{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndtyj-wJqGLQ",
        "outputId": "d88a895b-972f-475a-951d-d0ed09a6157f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: musdb in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from musdb) (1.26.4)\n",
            "Requirement already satisfied: stempeg>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from musdb) (0.2.3)\n",
            "Requirement already satisfied: pyaml in /usr/local/lib/python3.10/dist-packages (from musdb) (24.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from musdb) (4.66.5)\n",
            "Requirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from stempeg>=0.2.3->musdb) (0.2.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml->musdb) (6.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install musdb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import musdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MUSDB18 dataset\n",
        "def load_musdb_data(sample_rate=44100):\n",
        "    mus = musdb.DB(download=True, subsets='train')\n",
        "\n",
        "    mixtures = []\n",
        "    vocals = []\n",
        "    drums = []\n",
        "    bass = []\n",
        "    others = []\n",
        "\n",
        "    for track in mus:\n",
        "        audio, sr = track.audio, track.rate\n",
        "        if sr != sample_rate:\n",
        "            audio = librosa.resample(audio, sr, sample_rate)\n",
        "\n",
        "        mixtures.append(audio)\n",
        "        vocals.append(track.targets['vocals'].audio)\n",
        "        drums.append(track.targets['drums'].audio)\n",
        "        bass.append(track.targets['bass'].audio)\n",
        "        others.append(track.targets['other'].audio)\n",
        "\n",
        "    mixtures = np.array(mixtures)\n",
        "    vocals = np.array(vocals)\n",
        "    drums = np.array(drums)\n",
        "    bass = np.array(bass)\n",
        "    others = np.array(others)\n",
        "\n",
        "    return mixtures, vocals, drums, bass, others\n"
      ],
      "metadata": {
        "id": "uwYjTpLnrNez"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_spectrogram(audio, n_fft=1024, hop_length=512):\n",
        "    # Check if the audio length is less than n_fft\n",
        "    if len(audio) < n_fft:\n",
        "        # Pad audio with zeros if it's too short\n",
        "        audio = np.pad(audio, (0, n_fft - len(audio)), 'constant')\n",
        "\n",
        "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
        "    magnitude, phase = np.abs(stft), np.angle(stft)\n",
        "    return magnitude, phase\n",
        "\n",
        "\n",
        "def spectrogram_to_audio(magnitude, phase, hop_length=512):\n",
        "    stft_reconstructed = magnitude * np.exp(1j * phase)\n",
        "    audio_reconstructed = librosa.istft(stft_reconstructed, hop_length=hop_length)\n",
        "    return audio_reconstructed\n"
      ],
      "metadata": {
        "id": "Ehaa_9YOsXh_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_spectrograms(audio_list, n_fft=1024, hop_length=512):\n",
        "    magnitudes = []\n",
        "    phases = []\n",
        "\n",
        "    for audio in audio_list:\n",
        "        # Ensure the audio is at least as long as n_fft\n",
        "        if len(audio) < n_fft:\n",
        "            # Pad the audio to make sure it is long enough\n",
        "            audio = np.pad(audio, (0, n_fft - len(audio)), 'constant')\n",
        "\n",
        "        try:\n",
        "            mag, ph = audio_to_spectrogram(audio, n_fft, hop_length)\n",
        "        except Exception as e:\n",
        "            print(f'Error processing audio: {e}')\n",
        "            # Provide a default shape for cases where an exception occurs\n",
        "            mag, ph = np.zeros((n_fft//2 + 1, (len(audio) // hop_length) + 1)), np.zeros((n_fft//2 + 1, (len(audio) // hop_length) + 1))\n",
        "\n",
        "        magnitudes.append(mag)\n",
        "        phases.append(ph)\n",
        "\n",
        "    magnitudes = np.array(magnitudes)\n",
        "    phases = np.array(phases)\n",
        "    return magnitudes, phases\n"
      ],
      "metadata": {
        "id": "LOOQWWQttjcD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    conv1 = tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = tf.keras.layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')(pool1)\n",
        "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = tf.keras.layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')(pool2)\n",
        "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = tf.keras.layers.Conv2D(512, kernel_size=3, activation='relu', padding='same')(pool3)\n",
        "\n",
        "    up5 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv4)\n",
        "    up5 = tf.keras.layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')(up5)\n",
        "    up5 = tf.keras.layers.Concatenate()([up5, conv3])\n",
        "\n",
        "    up6 = tf.keras.layers.UpSampling2D(size=(2, 2))(up5)\n",
        "    up6 = tf.keras.layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')(up6)\n",
        "    up6 = tf.keras.layers.Concatenate()([up6, conv2])\n",
        "\n",
        "    up7 = tf.keras.layers.UpSampling2D(size=(2, 2))(up6)\n",
        "    up7 = tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')(up7)\n",
        "    up7 = tf.keras.layers.Concatenate()([up7, conv1])\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, kernel_size=1, activation='sigmoid')(up7)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "w1b6AxEjtrN7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ64n-zV7ZzQ",
        "outputId": "eab372d4-002b-4626-ec6e-01a71d468175"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Load MUSDB18 dataset\n",
        "mixtures, vocals, drums, bass, others = load_musdb_data()\n",
        "\n",
        "# Preprocess audio data to obtain spectrograms\n",
        "mixture_mags, mixture_phases = preprocess_spectrograms(mixtures)\n",
        "vocals_mags, _ = preprocess_spectrograms(vocals)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(mixture_mags, vocals_mags, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the input shape for the model\n",
        "input_shape = (mixture_mags.shape[1], mixture_mags.shape[2], 1)\n",
        "\n",
        "# Create and compile the U-Net model\n",
        "model = unet_model(input_shape)\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "plot_model(model,\n",
        "           show_shapes = True,\n",
        "           show_dtype=False,\n",
        "           show_layer_names = True,\n",
        "           rankdir = 'TB',\n",
        "           expand_nested = False,\n",
        "           dpi = 70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBSRBTzzttB0",
        "outputId": "eb375e01-3d0c-491a-9552-a64b3d919090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=2\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the U-Net model\n",
        "history = model.fit(X_train[..., np.newaxis], y_train[..., np.newaxis],\n",
        "                    validation_data=(X_val[..., np.newaxis], y_val[..., np.newaxis]),\n",
        "                    epochs=50, batch_size=8)"
      ],
      "metadata": {
        "id": "qCe2_5C8678e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_save_audio(test_file_path, model, n_fft=1024, hop_length=512):\n",
        "    test_mixture, sr = librosa.load(test_file_path, sr=None)\n",
        "    test_mixture = librosa.resample(test_mixture, sr, 44100)\n",
        "\n",
        "    if len(test_mixture) < n_fft:\n",
        "        test_mixture = np.pad(test_mixture, (0, n_fft - len(test_mixture)), 'constant')\n",
        "\n",
        "    test_mixture_mag, test_mixture_phase = audio_to_spectrogram(test_mixture, n_fft, hop_length)\n",
        "    test_mixture_mag = np.expand_dims(test_mixture_mag, axis=(0, -1))\n",
        "\n",
        "    predicted_vocal_mag = model.predict(test_mixture_mag)\n",
        "    predicted_vocal_audio = spectrogram_to_audio(predicted_vocal_mag.squeeze(), test_mixture_phase)\n",
        "\n",
        "    librosa.output.write_wav('predicted_vocal.wav', predicted_vocal_audio, sr=44100)\n",
        "\n",
        "predict_and_save_audio('path_to_test_mixture.wav', model)\n"
      ],
      "metadata": {
        "id": "KUgnQxsjyfIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JVBNEJXNyi8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMaaeFLIylcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}